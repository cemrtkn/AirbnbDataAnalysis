# -*- coding: utf-8 -*-
"""CS210Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19JBJyLdbYV94MM95d9VJUAxDdh3xblJF

# NYC AIRBNB DATA PROGRESS REPORT BY CEM ERTURKAN
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount("/content/gdrive", force_remount=True)

import pandas as pd  # an alias for pandas
import numpy as np
import matplotlib.pyplot as plt

# %matplotlib inline

from os.path import join 
data_path = "/content/gdrive/My Drive"
filename1 = "AB_NYC_2019.csv"

df = pd.read_csv(join(data_path, filename1), delimiter=",")

"""## **INTRODUCTION**"""

df.head()

"""### Objective:

The objective of this report is to present findings of the investigative studies done on the correlative relationships between the variables of our datasets. By doing so, I hope to create a strong basis to provide ease for mechanisms (which will be utilized in the following part of this project) to solve the main problem of this project which is to come up with the formula for possibly the most successful Airbnb listing for an investor.

### Datasets to be utilized:

1. NYC Airbnb listings dataset for 2019(*main dataset*)
2. NYC colleges and universities
3. NYC subway stations
4. The Points Of Interest (*points are a compilation of what the different city agencies consider to be a Common Place or Place/Point of Interest*)
5. Primary commercial zoning data

***Further explanations of the qualities of the datasets and the reasoning behind the selections of these datasets will be provided when the datasets are utilized.***

### Methods:
"""

df.dropna(how="any").sort_values(by='reviews_per_month', ascending=False)

"""## **ANALYSIS**

### Understanding the data:

Let's try to understand the data by looking at the basics such as :

####1. what the data tells us is
,by looking at the source, that "this data file includes all needed information to find out more about hosts, geographical availability, necessary metrics to make predictions and draw conclusions(about Airbnb listings)."

####2. **what's its qualitative and quantitative features**
"""

rows_cols = str(df.shape)
types_of_data = str(df.columns) + "\n"  + str(df.dtypes)

print(rows_cols + "\n"  + types_of_data)

"""####3. **Statistical analysis of the data**

which shows what the tendencies are as far as how the data is distributed. Provides a general sense of understanding of the data
"""

df.describe()

"""####**Dealing with missing data**

Since there is no missing data in the # of reviews category in any row, 10052 rows which doesn't have last review and review per month columns filled are those who are reviewed by no one, hence, it doesn't mean there is missing data in those two columns. However, for columns name and host nome, same thing cannot be said they seem to have missing data in their respective columns. Therefore, that should be taken into consideration when the names and the host names of the listings are investigated. 
"""

df.isnull().sum()

"""####**Categorization of prices according to the percentiles and mean and std**

Also, the data provided by https://www.renthop.com/average-rent-in/new-york-city-ny **show that rents for a studio apartment starts at 1700 dolars which is roughly the middle point of the below average categorization which I think is suitable.
"""

def price_range(a):
  if a < 50:
    return "cheap"
  if 49< a <80:
    return "below average"
  if 79 <a < 180:
    return "average"
  if 179< a <300 :
    return "expensive"
  if(299< a):
    return "deluxe"
 
df['price_range'] = df.apply(lambda row: price_range(row['price']), axis=1)
df

df[df['minimum_nights']>30].shape

"""**since there is only 747 listings with minimum nights requirement above 30 we are going to discard those in our counts depending on minimum nights histogram because their significance in the database is negligible and this discarding will provide us with a better visual representation**

### Visuals

#### Analysis of the visauls
"""

from scipy import stats
from collections import Counter

fig, axs = plt.subplots(2,2,figsize=(25,10))


#
counts1 = Counter(df['room_type'])
axs[0][0].bar(counts1.keys(), counts1.values())
axs[0][0].set_title("room type counts")

keys = Counter(df['price_range'])
counts2 = df['price_range'].value_counts(normalize=True) * 100

axs[0][1].bar(keys.keys(),counts2,color='green')
axs[0][1].set_title('price range percentages')#hist of this data

axs[1][0].hist(((df[(df['calculated_host_listings_count']<30)]['minimum_nights'])),bins = 40,rwidth=1,weights=(1/48895)*(df[(df['calculated_host_listings_count']<30)]['minimum_nights']),color = 'red')


#(df[(df['minimum_nights'] < 30)]['minimum_nights'])axs[1][1].hist(bins = 15,rwidth=1,weights=(0.01)*(df[(df['minimum_nights'] < 30)]['minimum_nights']),color = "purple")
axs[1][1].hist((df[(df['minimum_nights'] < 30)]['minimum_nights']),bins = 30,rwidth=1,weights=(0.01)*(df[(df['minimum_nights'] < 30)]['minimum_nights']),color = "purple")
axs[1][1].set_title('minimum nights count')#hist of this data

plt.show()

"""#####frequencies and counts of different variable counts
1.   Shared rooms occupy a very little percentage in the data. From one perspective it might means there is little demand in the market which yields a smaller supply. From another perspective,it means ,in other categories, there is much more competition for the investor.(Shared room concept is mostly considered as a pocket money income for financially challenged due to its low price and low demand, which is not the typical investor type so it is an unlikely option for an investment.)
2.   For me, the important thing in this data the percentage of expensive listings, although it is understandable due to NYC being notorious for one of the most expensive centers for accomodation.

3. Looking at the minimum nights regarding the supply-demand concept in economics, there is a tendency for the owners to list the houses/rooms available for stays between 1-7 days. It might be because they formulated a formula to optimize the amount of money they get according to the demands of the customer and their earnings.

#####Price range vs counts based on room types
"""

price_type = pd.crosstab(df["room_type"], df["price_range"])

r_t = price_type.loc["Entire home/apt"]
p_r = price_type.loc["Private room"]

fig, ax = plt.subplots(1, 1, figsize=(10,8))

index = np.arange(price_type.columns.shape[0])
bar_width = 0.35
opacity = 0.8
 
rects1 = plt.bar(index, r_t, bar_width,alpha=opacity,color='g',label='Entire home/apt')
 
rects2 = plt.bar(index + bar_width, p_r, bar_width,alpha=opacity,color='r',label='Private room')
 
plt.xlabel('room_type')
plt.xticks(index + bar_width - 0.2, price_type.columns)
plt.legend()
 
plt.tight_layout()
plt.show()

"""####4. how can we define success for an airbnb listing? 
Review is a huge part in both deciding your accomodation in airbnb and also interpreting how fruquently/much was this place booked by others.

Therefore, I decided to go with reviews per month variable to decide on the success of a listing(in terms of how frequently were they booked) because it was the only data that suggested anything about being booked other than the total number of reviews. Choosing it over number of reviews was due to the fact that total number of reviews was more accumulative than the reviews per month. (A listing that has been going for 4 months who had 20 reviews scores higher than one which was active for a week and had 7 reviews(both with the same requirement for minimum nights) although it was more succesful in terms of attracting customers)

"""

import seaborn as sns
from scipy import stats
from scipy import special

correct = df[(df["reviews_per_month"] <31)].dropna()

ba

fig, ax = plt.subplots(1, 1, figsize=(14,6))  
                                        
test_type = 'reviews_per_month'
ex = correct[correct["price_range"] == "expensive"][test_type]  
av = correct[correct["price_range"] == "average"][test_type] 
ba = correct[correct["price_range"] == "below average"][test_type]
ch = correct[correct["price_range"] == "cheap"][test_type]
de = correct[correct["price_range"] == "deluxe"][test_type]


sns.kdeplot(ex, shade=True, label="expensive", color="c")
sns.kdeplot(av, shade=True, label="average", color="m")
sns.kdeplot(ba, shade=True, label="below_average", color="y")
sns.kdeplot(ch, shade=True, label="cheap", color="g")
sns.kdeplot(de, shade=True, label="deluxe", color="r")

plt.suptitle("revies per month distribution")
plt.show()

stats.ttest_ind(de, ch, equal_var=False)

stats.ttest_ind(av, ch, equal_var=False)

stats.ttest_ind(ba, ch, equal_var=False)

stats.ttest_ind(ex, ch, equal_var=False)

stats.ttest_ind(ba, av, equal_var=False)

"""Taking significance level as 0.05 there is evidence that most favorable option for getting the most amount of reviews per month is having a listing price between 50 and 79 which was the below average price for our data set.(all the test above are significant and cheap options are only outperformed by the below average ones)"""

boroughs = correct['neighbourhood_group']

bronx = correct[correct["neighbourhood_group"] == "Bronx"][test_type]  # completed students
brooklyn = correct[correct["neighbourhood_group"] == "Brooklyn"][test_type] 
manhattan = correct[correct["neighbourhood_group"] == "Manhattan"][test_type]
queens = correct[correct["neighbourhood_group"] == "Queens"][test_type]
staten = correct[correct["neighbourhood_group"] == "Staten Island"][test_type]


sns.kdeplot(bronx, shade=True, label="Bronx", color="c")
sns.kdeplot(brooklyn, shade=True, label="Brooklyn", color="m")
sns.kdeplot(manhattan, shade=True, label="Manhattan", color="y")
sns.kdeplot(queens, shade=True, label="Queens", color="g")
sns.kdeplot(staten, shade=True, label="Staten Island", color="r")

plt.suptitle("revies per month distribution")
plt.show()

stats.ttest_ind(staten, manhattan, equal_var=False)

stats.ttest_ind(bronx, manhattan, equal_var=False)

stats.ttest_ind(brooklyn, manhattan, equal_var=False)

stats.ttest_ind(queens, manhattan, equal_var=False)

stats.ttest_ind(bronx, staten, equal_var=False)

stats.ttest_ind(queens, staten, equal_var=False)

stats.ttest_ind(bronx, queens, equal_var=False)

stats.ttest_ind(queens, brooklyn, equal_var=False)

stats.ttest_ind(bronx, brooklyn, equal_var=False)

stats.ttest_ind(staten, brooklyn, equal_var=False)

"""Looking at the p values Queens and Staten Island seems to be the two most frequently monthly reviewed boroughs."""

cols = ["longitude", "reviews_per_month"]

sns.pairplot(data=correct, vars=cols)

cols = ["latitude", "reviews_per_month"]

sns.pairplot(data=correct, vars=cols)

cols = ["price", "reviews_per_month"]

sns.pairplot(data=correct, vars=cols)

cols = ["minimum_nights", "reviews_per_month"]

sns.pairplot(data=correct, vars=cols)

corr_df = df.dropna(how="any").corr()
plt.imshow(corr_df, cmap="bwr", vmin=-1, vmax=1)
plt.colorbar()
plt.xticks(range(len(corr_df)),corr_df.columns, rotation=20)
plt.yticks(range(len(corr_df)),corr_df.index)
plt.show()

"""The correlation map shows, although not that strong, there is correlation between the variables of the data.

Due to the results of this two sample t test, there is evident difference between
"""

xd = 1000

g1 =  (df[(df['room_type']=='Private room') & (df['price']<xd) ]['price'])
g2 =  (df[(df['room_type']=='Entire home/apt') & (df['price']<xd) ]['price'])
g3 = (df[(df['room_type']=='Shared room') & (df['price']<xd) ]['price'])
g4 =  (df[(df['room_type']=='Private room') & (df['price']<xd) ]['reviews_per_month'])
g5 =  (df[(df['room_type']=='Entire home/apt') & (df['price']<xd)]['reviews_per_month'])
g6 = (df[(df['room_type']=='Shared room') & (df['price']<xd) ]['reviews_per_month'])
hıbıd = [g1, g2, g3]
y = [g4,g5,g6]
data = []
for i in range(3):
  d = {
      "x": hıbıd[i],
      "y": y[i]
  }
  data.append(d)
colors = ["b","g","y"]
labels = ['private room','Entire home/apt','Shared room']
for i, d in enumerate(data):
  plt.scatter(d["x"], d["y"], c=colors[i], alpha=0.3, label=labels[i])

plt.legend()
plt.show()

xd = 1000

g1 =  (df[(df['room_type']=='Private room') & (df['price']<xd) & (df['reviews_per_month']<31) ]['price'])
g2 =  (df[(df['room_type']=='Entire home/apt') & (df['price']<xd) & (df['reviews_per_month']<31) ]['price'])
g3 = (df[(df['room_type']=='Shared room') & (df['price']<xd) & (df['reviews_per_month']<31) ]['price'])
g4 =  (df[(df['room_type']=='Private room') & (df['price']<xd) & (df['reviews_per_month']<31)]['reviews_per_month'])
g5 =  (df[(df['room_type']=='Entire home/apt') & (df['price']<xd) & (df['reviews_per_month']<31)]['reviews_per_month'])
g6 = (df[(df['room_type']=='Shared room') & (df['price']<xd) & (df['reviews_per_month']<31) ]['reviews_per_month'])
x_s = [g1, g2, g3]
y_s = [g4,g5,g6]

fig_s, axs_s = plt.subplots(1,3,figsize=(20,8))
colors = ['y','r','b']
for i in range(3):
  axs_s[i].scatter(x_s[i], y_s[i], c=colors[i], alpha=0.3, label='Private room')

stats.ttest_ind(g4, g5, equal_var=False)

stats.ttest_ind(g6, g4, equal_var=False)

stats.ttest_ind(g6, g5, equal_var=False)

"""Shared rooms evidently is the group that have a higher mean review per month and the tests are significant. Private rooms are the second."""

import folium
lat = df["latitude"].mean()
lng = df["longitude"].mean()

"""####The distribution of listings on the map"""

m1 = folium.Map(location=[lat, lng], zoom_start=12, tiles = "Stamen Terrain")

listings = df[["latitude", "longitude"]].values
for l in listings[:10000]:
  folium.CircleMarker(location=l, radius=0.5, color="blue").add_to(m1)

m1

lat = df["latitude"].mean()
lng = df["longitude"].mean()

m = folium.Map(location=[lat, lng], zoom_start=12, tiles = "Stamen Terrain")

winter_locs = df[[['latitude', 'longitude']].values

"""#### Comparison with the supporting data:

For tourists points of interest is important
"""

filename2 = "Point_Of_Interest.csv"

pi = pd.read_csv(join(data_path, filename2), delimiter=",")

def long(a):
  return float(a[7:17])

def lat(a):
  counter = 0
  i = 0
  while (i < len(a)) & (counter <2):
    i += 1
    if(a[i] == ' '):
      counter +=1
  return float(a[i+1:i+11])
pi['longitude'] = pi.apply(lambda row: long(row['the_geom']), axis=1)
pi['latitude'] = pi.apply(lambda row: lat(row['the_geom']), axis=1)
pi

"""##### Points of interest """

m2 = folium.Map(location=[lat, lng], zoom_start=12, tiles = "Stamen Terrain")


points_of_interest = pi[["latitude", "longitude"]].values

for i in points_of_interest:
  folium.CircleMarker(location=i, radius=0.5, color="red").add_to(m2)

m2

filename3 = "DOITT_SUBWAY_STATION_01_13SEPT2010.csv"

sub = pd.read_csv(join(data_path, filename3), delimiter=",")
sub

def lat1(a):
  counter = 0
  i = 0
  while (i < len(a)) & (counter <2):
    i += 1
    if(a[i] == ' '):
      counter +=1
  return float(a[i+1:i+11])
sub['longitude'] = sub.apply(lambda row: long(row['the_geom']), axis=1)
sub['latitude'] = sub.apply(lambda row: lat1(row['the_geom']), axis=1)
sub

"""#####Subway stations"""

m3 = folium.Map(location=[lat, lng], zoom_start=12, tiles = "Stamen Terrain")



subway_s = sub[["latitude", "longitude"]].values

for h in subway_s:
  folium.CircleMarker(location=h, radius=0.5, color="green").add_to(m3)

m3

filename4 = "COLLEGE_UNIVERSITY.csv"

c_u = pd.read_csv(join(data_path, filename4), delimiter=",")
c_u

c_u['longitude'] = sub.apply(lambda row: long(row['the_geom']), axis=1)
c_u['latitude'] = sub.apply(lambda row: lat1(row['the_geom']), axis=1)
c_u

"""##### Colleges and Universities"""

m4 = folium.Map(location=[lat, lng], zoom_start=12, tiles = "Stamen Terrain")



college_uni = c_u[["latitude", "longitude"]].values

for h in college_uni:
  folium.CircleMarker(location=h, radius=0.5, color="green").add_to(m4)

m4

filename5 = "EDC_PrimaryZoningCommercial_001_14AUG2009 (1).csv"

c_z = pd.read_csv(join(data_path, filename4), delimiter=",")
c_z

c_z['longitude'] = sub.apply(lambda row: long(row['the_geom']), axis=1)
c_z['latitude'] = sub.apply(lambda row: lat1(row['the_geom']), axis=1)
c_z

"""##### Commercial Zoning"""

m6 = folium.Map(location=[lat, lng], zoom_start=12, tiles = "Stamen Terrain")
commercial_zoning = c_z[["latitude", "longitude"]].values
for x in commercial_zoning:
  folium.CircleMarker(location=x, radius=0.5, color="blue").add_to(m6)

m6

"""Visually all the data portrayed above have similar location distributions to the """

m = df['reviews_per_month'].mean()
m5 = folium.Map(location=[lat, lng], zoom_start=12, tiles = "Stamen Terrain")
above_average = df[df["reviews_per_month"]>m]
listings_a_a = above_average[["latitude", "longitude"]].values
for x in listings[:10000]:
  folium.CircleMarker(location=x, radius=0.5, color="blue").add_to(m5)

m5

"""As one can see the listings who get per month reviewed more than the mean are distributed very similarly to the maps of points of interest, colloges and universities,subway stations.

This, I think points to the ability of location-based supportive data provided in this analysis to provide reasons for what we consider as successful Airbnb listings in this data. This is mainly due to:

Airbnb being a temporary option for accomodations which means its main audience is the tourists and those who participate in work-related vacations(colleges and unis and commercial zoning). Hence, the need for transportation and tourist activies.

## SUMMARY

In the context of my data analysis which looks for the best features for a high achieving/successful Airbnb listing, as parameter of success I have used reviews per month variable of the listings.

Through the analysis of the data the findings are:

1. Below average and cheap rooms usually get more reviews per month
2. Although the finding that Queens outperformed all the other boroughs in reviews per mont category clashes with our finding in the maps, it is valuable. Staten Island was also well performing.
3. Shared rooms are the best performing in our success category.
4. The distribution of higher achieving listings on the map suggest there is relation between attracting those who are looking for temporary stays 
5. Rooms and apartments have much more competition amongst category than shared rooms.

##Future Objectives:

Determining the best variable features for the listing in a more determined fashion through the use of machine learning algorithms and ultimately coming up with the best possible profile.
"""